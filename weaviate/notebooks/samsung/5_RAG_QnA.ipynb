{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568228b-299d-4163-89ce-f75b9e12ff87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker gradio langchain==0.0.245 pypdf weaviate-client --force-reinstall --quiet \n",
    "%pip install dependencies/botocore-1.29.162-py3-none-any.whl dependencies/boto3-1.26.162-py3-none-any.whl dependencies/awscli-1.27.162-py3-none-any.whl --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7560e2-2143-4246-929c-265828a59b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab5aee-3f58-41fe-b9ce-b08d791c17b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import ast\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from typing import List\n",
    "import weaviate\n",
    "import sagemaker\n",
    "\n",
    "# langchain libraries\n",
    "from langchain import PromptTemplate\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_utils.base import ConversationalRetrievalChain\n",
    "from langchain.llms.sagemaker_endpoint import  SagemakerEndpoint, LLMContentHandler\n",
    "from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef840c79-46d7-4c64-b46a-b4993f153b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker_session.get_caller_identity_arn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0e158-e2b8-4953-829a-9f4870b4a41b",
   "metadata": {},
   "source": [
    "<mark>Define the load balancer for the Weaviate instance</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ae0e5-659c-47a5-b800-866f69cb5262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elb_endpoint = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f9148-9b6a-4a3f-a063-23e2abd762fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv_client = weaviate.Client(url=f\"http://{elb_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945794e-ea76-46cc-806b-535aa2e939eb",
   "metadata": {},
   "source": [
    "<mark>Optional but recommended: provide your langsmith API key</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767774e9-641e-4c57-bc72-5cf845254c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "langsmith_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556b58-8689-4f6a-9951-3174566a06c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"QA Chain - {today}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90181a9-e05a-4318-8cb7-530850ab2cde",
   "metadata": {},
   "source": [
    "<h1>Deploy SageMaker Endpoint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2225d22-573e-43cc-99c1-5806ca7c0dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'OpenAssistant/pythia-12b-sft-v8-7k-steps'\n",
    "num_gpus = 8\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "health_check_timeout = 600\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf76f66-497c-4dc9-bfbf-7dd9355b9d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': model_id,\n",
    "  'SM_NUM_GPUS': json.dumps(num_gpus), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(20024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(20048),  # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    " 'MAX_CONCURRENT_REQUESTS': json.dumps(1) # uncomment to limit OOM errors #https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_oa_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config,\n",
    "  #transformers_version=\"4.30.1\"\n",
    ")\n",
    "\n",
    "llm_oa_endpoint = llm_oa_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n",
    "\n",
    "llm_oa_endpoint_name = llm_oa_endpoint.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce41000-7226-4e2c-ba91-022db1220115",
   "metadata": {},
   "source": [
    "<h1>Define Langchain LLM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7274d-5ab6-43b5-ad0a-71deaff8ef1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenAssistant LLM\n",
    "\n",
    "oa_parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"return_full_text\": False,\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|endoftext|>\"]\n",
    "  }\n",
    "\n",
    "class OAContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": oa_parameters, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        result = response_json[0][\"generated_text\"]\n",
    "        try:\n",
    "            response = result.split('<|assistant|>')[1]\n",
    "        except:\n",
    "            response = result\n",
    "        return response\n",
    "\n",
    "\n",
    "llm_oa = SagemakerEndpoint(\n",
    "    endpoint_name=llm_oa_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=oa_parameters,\n",
    "    content_handler=OAContentHandler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35f0bb-8412-421f-837f-96d1e95bd4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_oa(\"<|prompter|>What day comes after Tuesday?<|endoftext|><|assistant|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cca25-3abf-45f7-9d77-34dab94b6323",
   "metadata": {},
   "source": [
    "<h1>Define Weaviate Retriever</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7d240-3b8a-4b58-8c7c-470f76955ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv_hybrid_retriever = WeaviateHybridSearchRetriever(\n",
    "    client=wv_client,\n",
    "    index_name=\"ManualContent\",\n",
    "    text_key=\"content\",\n",
    "    attributes=[\"model_names\", \"file\"],\n",
    "    create_schema_if_missing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff93c8-74a5-4188-be06-6c5dc0470e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv_hybrid_retriever.get_relevant_documents(\n",
    "    query=\"how do I unlock the screen?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbca1f7-0de5-46cc-8b3f-23234b8e7dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv_hybrid_retriever.get_relevant_documents(\n",
    "    query=\"how do I unlock the screen?\",\n",
    "    where_filter={\n",
    "        \"path\": [\"model_names\"],\n",
    "        \"operator\": \"Equal\",\n",
    "        \"valueText\": \"Galaxy S22\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc2dda-e4d2-4575-a614-b5f11bdb253f",
   "metadata": {},
   "source": [
    "<h1>Define QA Chain and Prompt</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e2a23-2af3-43e2-9cec-801db5b156c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Context for response\n",
    "OA_TEMPLATE = \"\"\"<|prompter|>You are a tech professional who is an expert in smartphones. The context provided is a portion from a smartphone's user manual.\n",
    "\n",
    "Use the following context to answer the user's question. Make sure to read all the context before providing an answer.  \n",
    "Only provide answers that are drawn directly from the context provided.  \n",
    "\n",
    "If you do not find reference to the question in the provided context, say 'Sorry, I do not find any reference to this question in the provided context'\n",
    "\n",
    "\\nContext:\\n{context}\\nQuestion: {question}<|endoftext|><|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QA_PROMPT = PromptTemplate(template=OA_TEMPLATE, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749a3e0-fa3c-4c10-a4cb-ac4bfce1eca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm_oa, retriever=wv_hybrid_retriever, verbose=True, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab4bff-6fb1-4d20-bd84-7edc5c547c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(qa_chain.combine_docs_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ec002-b901-40bb-9fa0-35995654e0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain.combine_docs_chain.llm_chain.prompt = QA_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f2e7-584e-4c4d-89c1-9d560180cb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(qa_chain.combine_docs_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0efee0-1a48-4d9f-80b6-2efebacc4f67",
   "metadata": {},
   "source": [
    "<h1>Create Gradio App</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b7725-d675-4db3-b6e4-b83e4505f8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find unique model names\n",
    "response = (\n",
    "    wv_client.query\n",
    "    .get(\"Manual\", ['model_names'])\n",
    "    .do()\n",
    ")\n",
    "\n",
    "# collect uniue model names\n",
    "model_names = [r['model_names'] for r in response['data']['Get']['Manual']]\n",
    "model_names = [item for sublist in model_names for item in sublist]\n",
    "model_names = list(set(model_names))\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de2bcf-237a-42e7-ac6e-a938ca7fc1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Samsung smartphone support service\")\n",
    "\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot()\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                message = gr.Textbox(label=\"Chat Message Box\", placeholder=\"Chat Message Box\", show_label=False)\n",
    "                model = gr.Dropdown(model_names, multiselect=False, label=\"Model\")\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    submit = gr.Button(\"Submit\")\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "    with gr.Column(visible=False) as resource_col:\n",
    "        resource_box = gr.Textbox(label=\"Resources\", interactive=False)\n",
    "\n",
    "\n",
    "    def respond(message, model, chat_history):\n",
    "        # convert chat history to prompt\n",
    "        history = []\n",
    "        if len(chat_history) > 0:\n",
    "            history = [(h[0], h[1]) for h in history]\n",
    "\n",
    "        # send request to endpoint\n",
    "        result = qa_chain({\"question\": {\"question\": message, \"model\": model}, \"chat_history\": history})\n",
    "        # parse response\n",
    "        parsed_response = result['answer']\n",
    "\n",
    "        sources_list = [r.metadata for r in result['source_documents']]\n",
    "\n",
    "        sources_str = ''\n",
    "\n",
    "        files = []\n",
    "        model_names = []\n",
    "        print(sources_list)\n",
    "        for source in sources_list:\n",
    "            file = source['file']\n",
    "            model_names.extend(source['model_names'])\n",
    "            model_names = list(set(model_names))\n",
    "            if file not in files:\n",
    "                sources_str += 'FILE: ' + file + '\\n'\n",
    "                files.append(file)\n",
    "\n",
    "        sources_str += 'MODELS: ' + ','.join(model_names)\n",
    "\n",
    "        history.append((message, parsed_response))\n",
    "\n",
    "        return \"\", history, resource_col.update(visible=True), resource_box.update(value=sources_str)\n",
    "\n",
    "    submit.click(fn=respond, inputs=[message, model, chatbot], outputs=[message, chatbot, resource_col, resource_box], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b33f79-28d7-4200-bcd8-62eb47a74246",
   "metadata": {},
   "source": [
    "<h2>Cleanup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601afabd-bf22-40bd-8cbd-d437aeb39661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(\n",
    "    EndpointName=llm_oa_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224faf8-0fee-464f-a3e4-e7bfa0299d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
