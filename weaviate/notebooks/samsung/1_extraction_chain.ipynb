{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7a010-5109-4317-88d0-4f714017045a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.245 unstructured tiktoken pypdf sagemaker --quiet\n",
    "%pip install dependencies/botocore-1.31.21-py3-none-any.whl dependencies/boto3-1.28.21-py3-none-any.whl dependencies/awscli-1.29.21-py3-none-any.whl --force-reinstall --quiet\n",
    "%pip install transformers==4.24.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056128d7-1233-4ee8-aaf7-1b9aee73b538",
   "metadata": {},
   "source": [
    "<mark>Restart the kernel after the install to make use of the updated packages</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3e9d6-0f23-4aaa-b3b7-152dd16f3234",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to collect the following pieces of information from user guides for Samsung smart phones.  You can review the user manuals that will be used for this excercise in the manuals/ folder.\n",
    "\n",
    "model_names: The model names covered by the manual.\\\n",
    "key_features: A summary of key features for the devices covered in the manual\\\n",
    "company_address: The address listed for Samsung\\\n",
    "document_summary: A summary of the document\\\n",
    "file: The file name for the manual\\\n",
    "stylus: whether or not the devices uses an \"S Pen\" stylus\n",
    "\n",
    "Once this information is collected, it will be loaded into the database and queried in the remaining notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c1aeb-51f1-47a8-afe6-e2d05b9b72ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefeb7e-b043-4dbc-a2c5-d2902717d16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import ast\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# sagemaker libraries\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# langchain libraries\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms.sagemaker_endpoint import  SagemakerEndpoint, LLMContentHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# bedrock libraries\n",
    "from bedrock_utils import bedrock, print_ww\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# langsmith libraries\n",
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dc2b9-7e81-4291-a951-bbdaa280ad14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08eee29-7484-4b99-88aa-770b80dde166",
   "metadata": {
    "tags": []
   },
   "source": [
    "<mark>Optional but recommended: provide your langsmith API key</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d6949-2de9-4cd7-8f45-7b9ad55bd72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langsmith_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002e700-798a-489b-9f7b-1ab6afb88b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# langsmith env vars\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Manuals Extraction - {today}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02519-ba71-4ce7-86ba-a255e689b55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(region=region,url_override=f'https://prod.{region}.frontend.bedrock.aws.dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37823221-668a-4571-a70f-655c15035a29",
   "metadata": {},
   "source": [
    "<h1>Deploy SageMaker Endpoint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ef114-acdc-43b1-b6e7-aa08cec4656a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'huggingface-llm-falcon-40b-instruct-bf16'\n",
    "num_gpus = 8\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "config = {\n",
    "  'SM_NUM_GPUS': json.dumps(num_gpus), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(10012),\n",
    "  'MAX_BATCH_PREFILL_TOKENS': json.dumps(10024),\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(10024),\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(10024),  # Max length of the generation (including input text)\n",
    "  'MAX_CONCURRENT_REQUESTS': json.dumps(1) # limit OOM errors https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de2167-1de1-4132-8437-2ad44c752bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model = JumpStartModel(model_id=model_id, env=config, instance_type=instance_type) \n",
    "predictor = my_model.deploy()\n",
    "\n",
    "llm_falcon_endpoint = predictor.endpoint_name\n",
    "llm_falcon_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96241b6d-86a9-4f92-9545-94c89dc233d9",
   "metadata": {},
   "source": [
    "<h1>Define Langchain LLMs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cf128-43b1-421d-8bed-4c6075348a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Titan LLM\n",
    "llm_titan = Bedrock(model_id=\"amazon.titan-tg1-large\", \n",
    "              model_kwargs ={\n",
    "                   \"maxTokenCount\": 4096,\n",
    "                   \"stopSequences\": [],\n",
    "                   \"temperature\":0,\n",
    "                   \"topP\":1\n",
    "                },\n",
    "              client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab852d8e-aab7-4dce-afb8-568517807cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Falcon LLM\n",
    "falcon_parameters = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.01, \n",
    "        \"max_new_tokens\": 2000,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "\n",
    "class FalconContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": falcon_parameters, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        result = response_json[0][\"generated_text\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "llm_falcon = SagemakerEndpoint(\n",
    "    endpoint_name=llm_falcon_endpoint,\n",
    "    region_name=region,\n",
    "    model_kwargs=falcon_parameters,\n",
    "    content_handler=FalconContentHandler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc60d4-a739-46ea-a847-4dfb370a370a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_titan(\"What day comes after Tuesday?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b681fca-76b5-43a9-be32-d3fc11859f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_falcon(\"What day comes after Tuesday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463df4e-66b9-446e-9377-08f2e448c1c7",
   "metadata": {},
   "source": [
    "<h1>Define \"refine\" QA Chains</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0aa9f2-6ebe-4ce6-886a-56be407172cc",
   "metadata": {},
   "source": [
    "QA Chains of type \"refine\" are themselves made up of two disinct chains: an \"initial\" chain and a \"refine\" chain.  \n",
    "\n",
    "You can think of an LLMChain as consisting of two components:\n",
    "- a language model\n",
    "- a prompt or prompts\n",
    "\n",
    "Each chain has an input_key (variable name for the text going into the prompt) and an output_key (variable name for the text coming out of the language model).\n",
    "\n",
    "The general idea is that we want to divide our underlying document into chunks.  The first chunk of the document will be passed into the \"initial\" chain to request an initial answer.  All remaining chunks will be passed to the \"refine\" chain along with the current response.  This allows the language model to refine the answer as it sees more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ff91f-a720-4f22-92ef-b57d74caf881",
   "metadata": {
    "tags": []
   },
   "source": [
    "![refine QA chain](images/refine-chain.drawio.png \"Refine QA Chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f4f8d-aa8c-4b3a-a59d-b7d15e5a827e",
   "metadata": {},
   "source": [
    "![refine QA chain](images/refine-qa-process.drawio.png \"Refine QA Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c30e3-c957-4f12-b2b9-af4624680154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can start with a base QA chain of type \"refine\" to review its components\n",
    "base_qa_chain = load_qa_chain(llm=llm_titan, chain_type='refine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c8748-ebce-4ae4-b79a-eadcc816eef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's review the \"initial\" chain's language model\n",
    "base_qa_chain.initial_llm_chain.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7704585-baa7-4669-94b9-50c8e738c732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and the \"initial\" chain's default prompt\n",
    "print(base_qa_chain.initial_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73dd673-108f-4bd7-aaf3-3254392ba07c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now the \"refine\" chain's language model\n",
    "base_qa_chain.refine_llm_chain.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c8f9-a946-49c8-ab56-c04559ad8738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and the \"refine\" chain's default prompt\n",
    "print(base_qa_chain.refine_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abee8d7-2e21-4bd7-9662-be25200a6a37",
   "metadata": {},
   "source": [
    "<h2>Define initial QA chains</h2>\n",
    "\n",
    "We will define distinct refine QA chains to collect:\n",
    "- models covered in each user manual\n",
    "- key features of the models discussed in each manul\n",
    "- the company address listed in the manuals\n",
    "\n",
    "After we define these QA chains, we can optionally customize the individual chains (initial and refine) that make up its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a035ae-a315-48f7-9bf9-a4f361a87a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa chains\n",
    "model_chain = load_qa_chain(llm=llm_titan, chain_type='refine', verbose=False)\n",
    "features_chain = load_qa_chain(llm=llm_titan, chain_type='refine', verbose=False)\n",
    "address_chain = load_qa_chain(llm=llm_titan, chain_type='refine', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b08ec-a6f2-4072-8025-1195462c3dd4",
   "metadata": {},
   "source": [
    "<h2>Customize Prompts</h2>\n",
    "Let's customize the prompts associated with the \"initial\" and \"refine\" chains to better reflect the information we are trying to collect.  We can:\n",
    "\n",
    "- assign the language model a persona \n",
    "- update the {question} input field so that we can pass a specific input question to each prompt\n",
    "- add additional instruction to the model to limit halucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc79b1-3ac6-44d8-9d8c-bd6da5c18256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_model_prompt = PromptTemplate.from_template(\"\"\"You are a tech professional who is an expert in smartphones.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context.\n",
    "Only return information that is directly derived from the context.\n",
    "\n",
    "Context information is below. \n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question: {model_question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a576814-5bc1-4c6b-a95f-cd33c16aabcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refine_model_prompt = PromptTemplate.from_template(\"\"\"You are a tech professional who is an expert in smartphones.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context.\n",
    "Only return information that is directly derived from the context.\n",
    "\n",
    "The original question is as follows: {model_question}\n",
    "\n",
    "We have provided an existing answer: {existing_answer}\n",
    "\n",
    "We have the opportunity to refine the existing answer(only if needed) with some more context below.\n",
    "------------\n",
    "{context_str}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a599c-a07f-403e-9d1b-839bd167fa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_features_prompt = PromptTemplate.from_template(\"\"\"You are a tech professional who is an expert in smartphones.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context.\n",
    "Only return information that is directly derived from the context.\n",
    "\n",
    "Context information is below. \n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question: {features_question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978f503-1e0b-45dc-bc71-b2a233f6bd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refine_features_prompt = PromptTemplate.from_template(\"\"\"You are a tech professional who is an expert in smartphones.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context.\n",
    "Only return information that is directly derived from the context.\n",
    "\n",
    "The original question is as follows: {features_question}\n",
    "\n",
    "We have provided an existing answer: {existing_answer}\n",
    "\n",
    "We have the opportunity to refine the existing answer(only if needed) with some more context below.\n",
    "------------\n",
    "{context_str}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640a8e9-3572-4ddc-9932-935566ab3556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_address_prompt = PromptTemplate.from_template(\"\"\"You are a helpful assistant who can help answer questions from the user.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context.\n",
    "\n",
    "Return only the address and no special characters.\n",
    "\n",
    "Context information is below. \n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question: {address_question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafac965-b693-40f3-8439-911dafb34f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refine_address_prompt = PromptTemplate.from_template(\"\"\"You are a helpful assistant who can help answer questions from the user.  Your task is to use the context provided to answer the question.\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  Do NOT make up any information that is not in the context. \n",
    "\n",
    "Return only the address and no special characters.\n",
    "\n",
    "The original question is as follows: {address_question}\n",
    "\n",
    "We have provided an existing answer: {existing_answer}\n",
    "\n",
    "We have the opportunity to refine the existing answer(only if needed) with some more context below.\n",
    "------------\n",
    "{context_str}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a567cf0-7a9d-4962-b75b-b648b1814782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update the chains to use the newly created prompts\n",
    "model_chain.initial_llm_chain.prompt = initial_model_prompt\n",
    "model_chain.refine_llm_chain.prompt = refine_model_prompt\n",
    "\n",
    "features_chain.initial_llm_chain.prompt = initial_features_prompt\n",
    "features_chain.refine_llm_chain.prompt = refine_features_prompt\n",
    "\n",
    "address_chain.initial_llm_chain.prompt = initial_address_prompt\n",
    "address_chain.refine_llm_chain.prompt = refine_address_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8d59b-0bed-4c31-b895-f5d1422686aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can review some of the changes that have been made\n",
    "print(model_chain.initial_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee5094-7085-49ef-b931-1e140df46ce7",
   "metadata": {},
   "source": [
    "<h2>Summary Chain</h2>\n",
    "\n",
    "\n",
    "The goal of the summary chain is to generate summaries of the individual documents.  Similar to the \"refine\" QA chain, a summary chain of type \"map reduce\" starts by breaking the document into chunks.  Each chunk is sent to the summarization chain individual to produce a summary of the chunk.  Next, each of the summaries are sent to the summarization chain together to generate a \"summary of the summaries\".\n",
    "\n",
    "As with the QA chains, we can customize the underling LLMChain to better meet our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd30c4e-e0e4-41c0-b02a-bf0725c7622e",
   "metadata": {},
   "source": [
    "![summary chain](images/summary-chain.drawio.png \"Summary Chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad2f32-c150-4c47-a3ca-41c9de82581e",
   "metadata": {
    "tags": []
   },
   "source": [
    "![summary chain process](images/summary-chain-process.drawio.png \"Summary Chain Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f45824-70bc-4d90-97b7-7f7ed6ca87e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate a summary chain\n",
    "summary_chain = load_summarize_chain(llm=llm_falcon, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b95a88-9b55-4fa3-86bb-07ae3b644df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the default prompt template\n",
    "print(summary_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b506f-a9ff-46c9-aa45-a742e29b7ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# customize the summary chain prompt template\n",
    "summary_chain_prompt = PromptTemplate.from_template(\"\"\"You are a tech professional who is an expert in smartphones.\n",
    "The context provided is a portion from a smartphone's user manual.\n",
    "\n",
    "If you do not find any relevant information to answer the question in the provided context, return 'NA'.  \n",
    "Only return information that is directly derived from the context.\n",
    "\n",
    "Write a very concise summary of the device described in the following section under the header \"Context:\".\n",
    "Please focus on the screen, camera, and battery life.\n",
    "\n",
    "Context:\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY:\"\"\")\n",
    "\n",
    "summary_chain.llm_chain.prompt = summary_chain_prompt\n",
    "\n",
    "print(summary_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99670007-4e6d-46ff-ac59-bae5a4d2f1f6",
   "metadata": {},
   "source": [
    "<h2>Collection Chain</h2>\n",
    "\n",
    "So far, we have the following chains to extract entities from our user manuals:\n",
    "\n",
    "- QA chain to collect the model names\n",
    "- QA chain to collect key features for the models in the manual\n",
    "- QA chain to collect the company address\n",
    "- Summary chain to generate a summary of the device\n",
    "\n",
    "Each of these chains will produce their own output, but what we really want is to consolidate all of the results into a single JSON object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bc74b-0edb-44ea-8537-4696a30c26ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, asign the output of each underlying chain an explicit name so that we can process its output\n",
    "model_chain.output_key = 'Model Names'\n",
    "features_chain.output_key = 'Key Features'\n",
    "address_chain.output_key = 'Company Address'\n",
    "summary_chain.output_key = 'Document Summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befc60b-92e1-41dd-8a76-bca0806ed942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next, we create a prompt template that expects the outputs of each chain as input\n",
    "collect_template = \"\"\"Here is information about a set of consumer electronic devices. \n",
    "\n",
    "Please respond by formatting this information in the specific schema described below.\n",
    "\n",
    "The response MUST be a dictionary with the following schema:\n",
    "{{\n",
    "    'model_names': list  // List of the models covered\n",
    "    'key_features': list  // List of key features of the models\n",
    "    'company_address': string  // Address for the company\n",
    "    'document_summary': string  // Summary of the document\n",
    "}}\n",
    "\n",
    "Respond ONLY with the json object and do not include any additional explanation.  Do NOT include any detail that is not privided in the context. \n",
    "\n",
    "---\n",
    "Example #1:\n",
    "\n",
    "Input:\n",
    "Here are the names of the devices:\n",
    "Model Name 1, Model Name 2, Model Name 3\n",
    "\n",
    "Here are some key features of the devices:\n",
    "- touch screen\n",
    "- water proof\n",
    "- fast charge\n",
    "\n",
    "Here is the Compnay Address:\n",
    "1234 Anywhere Lane, This Town, MO 64118\n",
    "\n",
    "Here is a summary of the document:\n",
    "This user manual provides instructions on how to use the HTC Model Name 1, Model Name 2, Model Name 3 phones, including sending texts, and using the apps.\n",
    "\n",
    "Response:\n",
    "{{\n",
    "    'model_names': ['Model Name 1', 'Model Name 2', 'Model Name 3'],\n",
    "    'key_features': ['touch screen','water proof','fast charge'],\n",
    "    'company_address': '1234 Anywhere Lane, This Town, MO 64118',\n",
    "    'document_summary': 'This user manual provides instructions on how to use the HTC Model Name 1, Model Name 2, Model Name 3 phones, including sending texts, and using the apps.'\n",
    "}}\n",
    "---\n",
    "\n",
    "Begin!\n",
    "\n",
    "Input:\n",
    "Here are the names of the models:\n",
    "{Model Names}\n",
    "\n",
    "Here are some key features of the models:\n",
    "{Key Features}\n",
    "\n",
    "Here is the Compnay Address:\n",
    "{Company Address}\n",
    "\n",
    "Here is a summary of the document:\n",
    "{Document Summary}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "#prompt_template = PromptTemplate(input_variables=[\"Model Names\", \"Key Features\", \"Company Address\", \"Document Summary\"], template=collect_template, partial_variables={\"format_instructions\": format_instructions})\n",
    "collect_prompt_template = PromptTemplate(input_variables=[\"Model Names\", \"Key Features\", \"Company Address\", \"Document Summary\"], template=collect_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a710b7c-a9cd-44d2-a442-a0516a7d6c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, create an LLM chain that uses this prompt template\n",
    "collection_chain = LLMChain(llm=llm_falcon, prompt=collect_prompt_template, output_key='collection') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b1603-18af-4267-ae57-1edecfdc5aa9",
   "metadata": {},
   "source": [
    "<h2>Sequential Chain</h2>\n",
    "\n",
    "Now it is time to tie everything together.  To begin, let's take a step to limit the number of calls that are made to our language models.  This will help to make this demo a little bit faster and more lightweight.  Instead of passing every chunk from every document, we're going to cheat a little bit by creating two groups.  More broadly, we have the ability to pass specific subsets of the underlying document into specific chains.\n",
    "\n",
    "- begin_documents: The first N chunks from the document.  This will serve as proxy for the entire document as a means to limit calls to the language model\n",
    "- address_documents: When searching for the address of the company, we know that the chunk of text will include the string \"address\".  So we create this group as a subset of all chunks that include the string \"address\" - again as a means to reduce the calls to the endpoint for the purpose of this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e6f40-e28b-4c44-8731-c2688473bf56",
   "metadata": {},
   "source": [
    "![sequential chain](images/collection-chain.drawio.png \"Sequential Chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7a36c-90d9-4be3-ab9c-d219a211df22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the input_key for each chain to expect the correct input group\n",
    "model_chain.input_key = 'begin_documents'\n",
    "features_chain.input_key = 'begin_documents'\n",
    "address_chain.input_key = 'address_documents'\n",
    "summary_chain.input_key = 'begin_documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824b1b0-eeb2-49ee-a1dc-d144716d575b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, create a sequential chain that combines all of the chains we have defined.  \n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[model_chain, features_chain, address_chain, summary_chain, collection_chain], \n",
    "    input_variables=[\"begin_documents\",\"address_documents\",'model_question','features_question','address_question'],\n",
    "    output_variables=['collection'],\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96c16-0d3c-4038-9723-050139083b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define questions to pass into each of our QA chains\n",
    "model_question = \"List the model names covered by this manual.\"\n",
    "features_question = \"\"\"List the top 5 key features for this device.  Do NOT return more than 5 features.\"\"\"\n",
    "address_question = \"\"\"What is the address in the context provided?  Please format your asnwer as:\n",
    "Street, City, State Zip Code\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ef38d-e015-4ab3-ab8a-2cfee31f51f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a text splitter\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2500, chunk_overlap=500, separators=[\" \", \",\", \"\\n\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d8255-2549-4539-a213-71465109bff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to process each manual\n",
    "def process_file(file):\n",
    "    # load PDF\n",
    "    loader = PyPDFLoader(f\"manuals/{file}\")\n",
    "\n",
    "    # split into chunks\n",
    "    texts = loader.load_and_split(text_splitter=recursive_text_splitter)\n",
    "\n",
    "    # define input\n",
    "    #address_texts = [t for t in texts if 'Address:' in t.page_content]\n",
    "    #address_texts = texts[-2:] # cheating again - we know the address is at the bottom of the document\n",
    "    begin_texts = texts[:2]\n",
    "\n",
    "    # replace newline characters\n",
    "    for i in range(len(texts)):\n",
    "        texts[i].page_content = texts[i].page_content.replace('\\n', '')\n",
    "\n",
    "    address_texts = [t for t in texts if 'Address:' in t.page_content]\n",
    "\n",
    "\n",
    "    # perform a simple string search to confirm whether there is discussion of an S Pen in the manual\n",
    "    stylus_texts = [t for t in texts if 'S Pen' in t.page_content]\n",
    "    if len(stylus_texts) == 0:\n",
    "        stylus = False\n",
    "    else:\n",
    "        stylus = True\n",
    "\n",
    "    # process file\n",
    "    sequential_result = sequential_chain({\"begin_documents\": begin_texts, \n",
    "                                      \"address_documents\": address_texts,\n",
    "                                      \"model_question\": model_question, \n",
    "                                      \"features_question\":features_question,  \n",
    "                                      \"address_question\":address_question,\n",
    "                                     },\n",
    "                                     return_only_outputs=True)\n",
    "\n",
    "    # convert to dictionary\n",
    "    response = sequential_result['collection'].strip()\n",
    "    response_dict = ast.literal_eval(response)\n",
    "\n",
    "    # add filename to the dictionary\n",
    "    response_dict['file'] = file\n",
    "\n",
    "    # add sylus boolean\n",
    "    response_dict['stylus'] = stylus\n",
    "\n",
    "    # write to disk\n",
    "    with open('manual_metadata.jsonl', 'a') as file:\n",
    "        file.write(json.dumps(response_dict))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a452a99-ea00-46df-a7ec-891b4683dc60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect input files\n",
    "dir_list = os.listdir('manuals/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce7590-8cf4-47db-8d91-88f46d7e92a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.debug=False # toggle for detailed logging\n",
    "\n",
    "for file in dir_list:\n",
    "    try:\n",
    "        print(f'Begin processing {file}')\n",
    "        process_file(file)\n",
    "        print(f'Successfully processed {file}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {file}')\n",
    "        print(e)\n",
    "    if file != dir_list[-1]:\n",
    "        time.sleep(60*2) # avoid throttling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3f373-a0ba-4d58-ba24-513b5a9b9e78",
   "metadata": {},
   "source": [
    "<h2>Cleanup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7a371-f7eb-4b53-ba81-f854b3cef29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(\n",
    "    EndpointName=llm_falcon_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1725ed-e8fd-4247-84aa-6c2069bcfee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
